{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'ChatCompletion'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m   \u001b[39mreturn\u001b[39;00m response\n\u001b[0;32m     16\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mAYOOOOOOO WHATS THE FIRST 4 DECIMALS OF PI?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 18\u001b[0m response \u001b[39m=\u001b[39m chat_response(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m     20\u001b[0m \u001b[39mprint\u001b[39m(response)\n",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m, in \u001b[0;36mchat_response\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mchat_response\u001b[39m(\u001b[39minput\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m   response_dict \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39mcreate(\n\u001b[0;32m      5\u001b[0m     model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     messages\u001b[39m=\u001b[39m[\n\u001b[0;32m      7\u001b[0m           {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mYou are a helpful assistant.\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m      8\u001b[0m           {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mWho won the world series in 2020?\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m      9\u001b[0m           {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mThe Los Angeles Dodgers won the World Series in 2020.\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m     10\u001b[0m           {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39minput\u001b[39m}\n\u001b[0;32m     11\u001b[0m       ]\n\u001b[0;32m     12\u001b[0m   )\n\u001b[0;32m     13\u001b[0m   response \u001b[39m=\u001b[39m response_dict[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     14\u001b[0m   \u001b[39mreturn\u001b[39;00m response\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'openai' has no attribute 'ChatCompletion'"
     ]
    }
   ],
   "source": [
    "# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\n",
    "import openai\n",
    "def chat_response(input):\n",
    "  response_dict = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "          {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "          {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "          {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "          {\"role\": \"user\", \"content\": input}\n",
    "      ]\n",
    "  )\n",
    "  response = response_dict[\"choices\"][0][\"message\"][\"content\"]\n",
    "  return response\n",
    "\n",
    "input = \"AYOOOOOOO WHATS THE FIRST 4 DECIMALS OF PI?\"\n",
    "\n",
    "response = chat_response(input)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openai_response(input):\n",
    "    response_dict = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=input+\"\\n\",\n",
    "    temperature=0.9,\n",
    "    max_tokens=10,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.0,\n",
    "    presence_penalty=0.6,\n",
    "    )\n",
    "\n",
    "    response = response_dict[\"choices\"][0][\"text\"].strip(\"\\n\")\n",
    "    completion_tokens = response_dict[\"usage\"][\"completion_tokens\"]\n",
    "    prompt_tokens = response_dict[\"usage\"][\"prompt_tokens\"]\n",
    "\n",
    "    return response, completion_tokens, prompt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openai_chat(input, history=None):\n",
    "    prompt = input if history is None else input+\"\\n\".join(history)\n",
    "    print(\"Prompt: \", prompt)\n",
    "    response, completion_tokens, prompt_tokens = openai_response(input)\n",
    "\n",
    "    # Convert the history to tuples\n",
    "    history_tuples = None if history is None else list(zip(history[0::2], history[1::2]))\n",
    "\n",
    "    # Convert lines to list of tuples\n",
    "    response_tuples = [(input, response)] if history is None else [(input, response)].extend(history_tuples)\n",
    "    print(response_tuples)\n",
    "\n",
    "    # Combine the history and the new response\n",
    "    new_history = history.extend([item for tuple in response_tuples for item in tuple])\n",
    "\n",
    "    return response_tuples, new_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  Hello\n",
      "[('Hello', 'Hi there! How are you?')]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'extend'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response_tuples, new_history \u001b[39m=\u001b[39m openai_chat(\u001b[39m\"\u001b[39;49m\u001b[39mHello\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mresponse_tuples: \u001b[39m\u001b[39m\"\u001b[39m, response_tuples)\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mnew_history: \u001b[39m\u001b[39m\"\u001b[39m, new_history)\n",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m, in \u001b[0;36mopenai_chat\u001b[1;34m(input, history)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(response_tuples)\n\u001b[0;32m     13\u001b[0m \u001b[39m# Combine the history and the new response\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m new_history \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39;49mextend([item \u001b[39mfor\u001b[39;00m \u001b[39mtuple\u001b[39m \u001b[39min\u001b[39;00m response_tuples \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m \u001b[39mtuple\u001b[39m])\n\u001b[0;32m     16\u001b[0m \u001b[39mreturn\u001b[39;00m response_tuples, new_history\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'extend'"
     ]
    }
   ],
   "source": [
    "response_tuples, new_history = openai_chat(\"Hello\")\n",
    "\n",
    "print(\"response_tuples: \", response_tuples)\n",
    "print(\"new_history: \", new_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  Hello againHello\n",
      "How can I help you?\n",
      "history is NOT None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response_tuples, new_history \u001b[39m=\u001b[39m openai_chat(\u001b[39m\"\u001b[39;49m\u001b[39mHello again\u001b[39;49m\u001b[39m\"\u001b[39;49m, new_history)\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mresponse_tuples: \u001b[39m\u001b[39m\"\u001b[39m, response_tuples)\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mnew_history: \u001b[39m\u001b[39m\"\u001b[39m, new_history)\n",
      "Cell \u001b[1;32mIn[33], line 15\u001b[0m, in \u001b[0;36mopenai_chat\u001b[1;34m(input, history)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# Combine the history and the new response\u001b[39;00m\n\u001b[0;32m     14\u001b[0m new_history \u001b[39m=\u001b[39m [] \u001b[39mif\u001b[39;00m history \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m history\n\u001b[1;32m---> 15\u001b[0m new_history\u001b[39m.\u001b[39mextend([item \u001b[39mfor\u001b[39;00m \u001b[39mtuple\u001b[39m \u001b[39min\u001b[39;00m response_tuples \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m \u001b[39mtuple\u001b[39m])\n\u001b[0;32m     17\u001b[0m \u001b[39mreturn\u001b[39;00m response_tuples, new_history\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "response_tuples, new_history = openai_chat(\"Hello again\", new_history)\n",
    "\n",
    "print(\"response_tuples: \", response_tuples)\n",
    "print(\"new_history: \", new_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  gfdg\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Viktor\\AppData\\Roaming\\Python\\Python311\\site-packages\\gradio\\routes.py\", line 374, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Viktor\\AppData\\Roaming\\Python\\Python311\\site-packages\\gradio\\blocks.py\", line 1017, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Viktor\\AppData\\Roaming\\Python\\Python311\\site-packages\\gradio\\blocks.py\", line 835, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Viktor\\AppData\\Roaming\\Python\\Python311\\site-packages\\anyio\\to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Viktor\\AppData\\Roaming\\Python\\Python311\\site-packages\\anyio\\_backends\\_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Viktor\\AppData\\Roaming\\Python\\Python311\\site-packages\\anyio\\_backends\\_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Viktor\\AppData\\Local\\Temp\\ipykernel_12704\\4288283100.py\", line 15, in openai_chat\n",
      "    new_history.extend([item for tuple in response_tuples for item in tuple])\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not iterable\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    state = gr.State([])\n",
    "\n",
    "    with gr.Row():\n",
    "        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(container=False)\n",
    "\n",
    "    txt.submit(openai_chat, [txt, state], [chatbot, state])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
